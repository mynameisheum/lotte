{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "데이터 분석"
      ],
      "metadata": {
        "id": "HJgsXnNJyXHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "qq76jFO_udjW",
        "outputId": "1d3f3970-4e4a-4cb4-da44-c24df7b47a4a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/롯데정보통신ai/1_인텐트_분류.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-13026ba106ff>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/롯데정보통신ai/1_인텐트_분류.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/롯데정보통신ai/1_인텐트_분류.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "data_path = '/content/drive/MyDrive/롯데정보통신ai/1_인텐트_분류.json'\n",
        "\n",
        "with open(data_path,'r') as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "data_type = []\n",
        "for data in json_data:\n",
        "    data_type.append(data['인텐트'])\n",
        "\n",
        "# label 유형\n",
        "print(set(data_type)) # {'결제_일반_질문', '결제_할인_질문', '구매_예약_요청', '배송_날짜_요청', '제품_시용_질문', '제품_정보_질문'}\n",
        "\n",
        "for i in list(set(data_type)): # i[0] = 90, i[1] = 133, i[2] = 154, i[3] = 78, i[4] = 68, i[5] = 110\n",
        "    print(f'{i}의 데이터 갯수 {data_type.count(i)}')\n",
        "\n",
        "# data 개수\n",
        "len(json_data) # 633\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_type = []\n",
        "for data in json_data:\n",
        "    data_type.append(data['발화자'])\n",
        "\n",
        "# label 유형\n",
        "print(set(data_type)) # {'결제_일반_질문', '결제_할인_질문', '구매_예약_요청', '배송_날짜_요청', '제품_시용_질문', '제품_정보_질문'}\n",
        "\n",
        "for i in list(set(data_type)): # i[0] = 90, i[1] = 133, i[2] = 154, i[3] = 78, i[4] = 68, i[5] = 110\n",
        "    print(f'{i}의 데이터 갯수 {data_type.count(i)}')\n",
        "\n",
        "# data 개수\n",
        "len(json_data) # 633"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "hYxXV99T1-q0",
        "outputId": "63d9f1ff-7468-411e-c8e5-9d8d4897b46d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'json_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b1bb0a559806>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'발화자'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# label 유형\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "중복 제거"
      ],
      "metadata": {
        "id": "TPZ4DC260xva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "json2pd = pd.DataFrame(json_data)\n",
        "print(json2pd['발화문'].nunique())\n",
        "print(json2pd.drop_duplicates(subset='발화문'))"
      ],
      "metadata": {
        "id": "T7rM2DjUzsnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bert를 이용한 classification model"
      ],
      "metadata": {
        "id": "xsrWF2uuya5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWgD-pWgyWa8",
        "outputId": "32da0deb-1c09-4784-9049-7558953dcc48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature가 2개니까 multi-task-learning으로 가자잇\n",
        "1. 소 의도분류\n",
        "2. 대 의도분류\n",
        "3. user 구분\n",
        "\n",
        "json-> tokenizer하기 위한 전처리"
      ],
      "metadata": {
        "id": "hSemPMgt_RTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "total_data = []\n",
        "\n",
        "# train # 9\n",
        "# test # 1\n",
        "\n",
        "for data in json_data:\n",
        "    temp = []\n",
        "    temp.append([data['발화자']][0])\n",
        "    # if data['발화자'] == 'c':\n",
        "        # temp.append(10)\n",
        "    # if data['발화자'] == 's':\n",
        "        # temp.append(11)\n",
        "    temp.append([data['발화문']][0])\n",
        "    if data['인텐트'] == '결제_일반_질문':\n",
        "        temp.append(0)\n",
        "    if data['인텐트'] == '결제_할인_질문':\n",
        "        temp.append(1)\n",
        "    if data['인텐트'] == '구매_예약_요청':\n",
        "        temp.append(2)\n",
        "    if data['인텐트'] == '배송_날짜_요청':\n",
        "        temp.append(3)\n",
        "    if data['인텐트'] == '제품_시용_질문':\n",
        "        temp.append(4)\n",
        "    if data['인텐트'] == '제품_정보_질문':\n",
        "        temp.append(5)\n",
        "    total_data.append(temp)"
      ],
      "metadata": {
        "id": "DPgb4T4S1Vol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "15b2f6e7-a258-484a-a536-e7b8c1e2519a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'json_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a91f953db1f7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# test # 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'발화자'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_data)\n",
        "print(total_data)"
      ],
      "metadata": {
        "id": "ZgAxBkY2F_b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(total_data, test_size = 0.1, shuffle = True, random_state = 42)"
      ],
      "metadata": {
        "id": "3GP2rFtBG70w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "pWHI5XggHOZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "bdqzP8vQBtkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "3ngTv2vAXRxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "print(okt.morphs('네, 삼성건조기 설치하러 11월 1일에 방문할 예정입니다.',stem = True))\n",
        "a = okt.morphs('네, 삼성건조기 설치하러 11월 1일에 방문할 예정입니다.',stem = True)"
      ],
      "metadata": {
        "id": "oQd3HwbXMZWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BertForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)"
      ],
      "metadata": {
        "id": "3vOk-0JNTnMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "model_name = 'klue/bert-base'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "print(tokenizer('네, 삼성건조기 설치하러 11월 1일에 방문할 예정입니다.', truncation=True, padding =True))\n",
        "print(tokenizer.tokenize('네, 삼성건조기 설치하러 11월 1일에 방문할 예정입니다.', truncation=True, padding =True))"
      ],
      "metadata": {
        "id": "LoGxdp8sXAqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_tokenize = []\n",
        "# for i,j,k in train_data:\n",
        "#     train_tokenize.append(tokenizer(j,truncation = True, padding = True))\n",
        "#     # print(i,j,k)\n",
        "\n",
        "# print(train_tokenize)"
      ],
      "metadata": {
        "id": "s6weWxdmBtBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data2feature(datas, max_length, tokenizer):\n",
        "    input_ids_features, attention_mask_features, token_type_ids_features, label_id_features = [], [], [], []\n",
        "\n",
        "    count = 0\n",
        "    for use,utt,lab in datas:\n",
        "        count+= 1\n",
        "        print(utt)\n",
        "        # klue tokenize로 치환\n",
        "        input_ids = tokenizer(utt)['input_ids'][:max_length]\n",
        "        # print(input_ids)\n",
        "        # print(input_ids[-1])\n",
        "        # print([tokenizer.sep_token])\n",
        "        input_ids[-1] =  3\n",
        "        token_type_ids = tokenizer(utt)['token_type_ids'][:max_length]\n",
        "        attention_masks = tokenizer(utt)['attention_mask'][:max_length]\n",
        "\n",
        "        # padding 생성\n",
        "\n",
        "        padding = [0] * (max_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        token_type_ids += padding\n",
        "        attention_masks += padding\n",
        "        assert max_length == len(input_ids) == len(token_type_ids) == len(attention_masks)\n",
        "        label_id = lab\n",
        "\n",
        "        # 변환한 데이터를 각 리스트에 저장\n",
        "        input_ids_features.append(input_ids)\n",
        "        attention_mask_features.append(token_type_ids)\n",
        "        token_type_ids_features.append(attention_masks)\n",
        "        label_id_features.append(label_id)\n",
        "        # print(input_ids_features)\n",
        "\n",
        "    # 변환한 데이터를 Tensor 객체에 담아 반환\n",
        "    # print(input_ids_features)\n",
        "    input_ids_features = torch.tensor(input_ids_features, dtype=torch.long)\n",
        "    attention_mask_features = torch.tensor(attention_mask_features, dtype=torch.long)\n",
        "    token_type_ids_features = torch.tensor(token_type_ids_features, dtype=torch.long)\n",
        "    label_id_features = torch.tensor(label_id_features, dtype=torch.long)\n",
        "\n",
        "    return input_ids_features, attention_mask_features, token_type_ids_features, label_id_features"
      ],
      "metadata": {
        "id": "SvnBTb-YfIDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "convert_data2feature(train_data, 64, tokenizer)\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "id": "8hTssIZjgILf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "class IntentClassifier(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(IntentClassifier, self).__init__(config)\n",
        "\n",
        "        # BERT 모델\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        # 히든 사이즈\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        # 분류할 라벨의 개수\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "\n",
        "        # self.linear = nn.Linear(in_features=self.hidden_size, out_features=self.num_labels)\n",
        "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features = 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=self.num_labels)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        # BERT 출력에서 CLS에 대응하는 벡터 표현 추출\n",
        "        # 선형 함수를 사용하여 예측 확률 분포로 변환\n",
        "\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
        "\n",
        "        # (batch_size, max_length, hidden_size)\n",
        "        bert_output = outputs[0] # CLS의 output\n",
        "\n",
        "        # (batch_size, hidden_size)\n",
        "        cls_vector = bert_output[:,0,:]\n",
        "\n",
        "        # class_output : (batch_size, num_labels)\n",
        "        cls_output = self.fc1(cls_vector)\n",
        "        cls_output = self.relu(cls_output)\n",
        "\n",
        "        cls_output = self.fc2(cls_output)\n",
        "        # cls_output = self.softmax(cls_output)\n",
        "\n",
        "        return cls_output"
      ],
      "metadata": {
        "id": "NcEvwlYbeQvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import (DataLoader, TensorDataset, RandomSampler)\n",
        "from transformers import BertConfig\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def train(config):\n",
        "    train_input_ids_features, train_attention_mask_features, train_token_type_ids_features, train_label_id_features = convert_data2feature(train_data, 64, tokenizer)\n",
        "    train_features = TensorDataset(train_input_ids_features, train_attention_mask_features, train_token_type_ids_features, train_label_id_features)\n",
        "    train_dataloader = DataLoader(train_features, sampler=RandomSampler(train_features), batch_size = config['batch_size'])\n",
        "\n",
        "    # test_input_ids_features, test_attention_mask_features, test_token_type_ids_features, test_label_id_features = convert_data2feature(test_data, 64, tokenizer)\n",
        "    # test_features = TensorDataset(test_input_ids_features, test_attention_mask_features, test_token_type_ids_features, test_label_id_features)\n",
        "    # test_dataloader = DataLoader(test_features, sampler=RandomSampler(test_features), batch_size = config['batch_size'])\n",
        "\n",
        "    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path='klue/bert-base')\n",
        "    setattr(bert_config, \"num_labels\", config[\"num_labels\"])\n",
        "\n",
        "    # model = IntentClassifier.from_pretrained(pretrained_model_name_or_path='klue/bert-base', config=bert_config).cuda()\n",
        "    model = IntentClassifier.from_pretrained(pretrained_model_name_or_path='klue/bert-base', config=bert_config)\n",
        "    # loss를 계산하기 위한 함수\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 모델 학습을 위한 optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # 모델의 정확도를 저장하기 위한 변수\n",
        "    max_accuracy = 0\n",
        "    global_step = 0\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "        model.train()\n",
        "\n",
        "        total_loss = []\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # batch = tuple(t.cuda() for t in batch)\n",
        "            batch = tuple(t for t in batch)\n",
        "            input_ids, attention_mask, token_type_ids, label_id = batch\n",
        "\n",
        "            # 역전파 단계를 실행하기 전에 변화도를 0으로 변경\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 모델 예측 결과\n",
        "            hypothesis = model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "            # loss 계산\n",
        "            loss = loss_func(hypothesis, label_id)\n",
        "\n",
        "            total_loss.append(loss.data.item())\n",
        "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
        "            loss.backward()\n",
        "            # 모델 내부 각 매개변수 가중치 갱신\n",
        "            optimizer.step()\n",
        "\n",
        "            # batch 당 저장할게\n",
        "            print(\"Average loss : {}\".format(np.mean(total_loss)))\n",
        "\n",
        "        # epoch 당 저장할게\n",
        "        bert_config.save_pretrained(save_directory=config[\"output_dir_path\"])\n",
        "        model.save_pretrained(save_directory=config[\"output_dir_path\"])"
      ],
      "metadata": {
        "id": "l6GdD288mopq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(config):\n",
        "\n",
        "    from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "    model_name = 'klue/bert-base'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    test_input_ids_features, test_attention_mask_features, test_token_type_ids_features, test_label_id_features = convert_data2feature(test_data, 64, tokenizer)\n",
        "    test_features = TensorDataset(test_input_ids_features, test_attention_mask_features, test_token_type_ids_features, test_label_id_features)\n",
        "    test_dataloader = DataLoader(test_features, sampler=RandomSampler(test_features), batch_size = config['batch_size'])\n",
        "\n",
        "    # train_input_ids_features, train_attention_mask_features, train_token_type_ids_features, train_label_id_features = convert_data2feature(train_data, 64, tokenizer)\n",
        "    # train_features = TensorDataset(train_input_ids_features, train_attention_mask_features, train_token_type_ids_features, train_label_id_features)\n",
        "    # train_dataloader = DataLoader(train_features, sampler=RandomSampler(train_features), batch_size = config['batch_size'])\n",
        "\n",
        "\n",
        "\n",
        "    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path=config['output_dir_path'])\n",
        "    model = IntentClassifier.from_pretrained(pretrained_model_name_or_path='/content/drive/MyDrive/롯데정보통신ai/model.safetensors', config=bert_config).cuda()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_hypothesis, total_labels = [], []\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "        input_ids, attention_mask, token_type_ids, label_id = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 모델 예측 결과\n",
        "            # hypothesis : (batch_size, num_labels)\n",
        "            hypothesis = model(input_ids, attention_mask, token_type_ids)\n",
        "            # 모델의 출력값에 softmax와 argmax 함수를 적용\n",
        "            hypothesis = torch.argmax(hypothesis, dim=-1)\n",
        "\n",
        "        # Tensor를 리스트로 변경\n",
        "        hypothesis = hypothesis.cpu().detach().numpy().tolist()\n",
        "        label_id = label_id.cpu().detach().numpy().tolist()\n",
        "\n",
        "        total_hypothesis.extend(hypothesis)\n",
        "        total_labels.extend(label_id)\n",
        "\n",
        "        for index in range(len(input_ids)):\n",
        "            input_tokens = tokenizer.convert_ids_to_tokens(input_ids[index])\n",
        "            print(input_tokens)\n",
        "            input_sequence = tokenizer.convert_tokens_to_string(input_tokens[1:input_tokens.index(tokenizer.sep_token)])\n",
        "            print(input_tokens)\n",
        "            print(input_sequence)\n",
        "\n",
        "            predict = [hypothesis[index]]\n",
        "            correct = [label_id[index]]\n",
        "\n",
        "            # if hypothesis[index] == 0:\n",
        "                # predict = '결제_일반_질문'\n",
        "\n",
        "            print(\"입력 : {}\".format(input_sequence))\n",
        "            print(\"출력 : {}, 정답 : {}\\n\".format(predict, correct))\n",
        "\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        print(\"Accuracy= {0:f}\\n\".format(accuracy_score(total_hypothesis, total_labels)))"
      ],
      "metadata": {
        "id": "Hrm9i5anxXPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"mode\": \"train\",\n",
        "            # \"train_data_path\": os.path.join(root_dir, \"train_datas_wordpiece.txt\"),\n",
        "            # \"test_data_path\": os.path.join(root_dir, \"test_datas_wordpiece.txt\"),\n",
        "            \"output_dir_path\":'/content/drive/MyDrive/롯데정보통신ai',\n",
        "            # \"cache_dir_path\": cache_dir,\n",
        "            # \"pretrained_model_name_or_path\": \"monologg/kobert\",\n",
        "            # \"label_vocab_data_path\": os.path.join(root_dir, \"label_vocab.txt\"),\n",
        "            \"num_labels\": 6,\n",
        "            \"max_length\": 64,\n",
        "            \"epoch\":1,\n",
        "            \"batch_size\":64,\n",
        "            }\n",
        "\n",
        "if (config[\"mode\"] == \"train\"):\n",
        "    train(config)\n",
        "else:\n",
        "    test(config)"
      ],
      "metadata": {
        "id": "hpiRne5E2UkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"mode\": \"test\",\n",
        "            # \"train_data_path\": os.path.join(root_dir, \"train_datas_wordpiece.txt\"),\n",
        "            # \"test_data_path\": os.path.join(root_dir, \"test_datas_wordpiece.txt\"),\n",
        "            \"output_dir_path\":'/content/drive/MyDrive/롯데정보통신ai',\n",
        "            # \"cache_dir_path\": cache_dir,\n",
        "            # \"pretrained_model_name_or_path\": \"monologg/kobert\",\n",
        "            # \"label_vocab_data_path\": os.path.join(root_dir, \"label_vocab.txt\"),\n",
        "            \"num_labels\": 6,\n",
        "            \"max_length\": 64,\n",
        "            \"epoch\":1,\n",
        "            \"batch_size\":64,\n",
        "            }\n",
        "\n",
        "if (config[\"mode\"] == \"train\"):\n",
        "    train(config)\n",
        "else:\n",
        "    test(config)"
      ],
      "metadata": {
        "id": "sEFfEJio5Ssf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"mode\": \"test\",\n",
        "            # \"train_data_path\": os.path.join(root_dir, \"train_datas_wordpiece.txt\"),\n",
        "            # \"test_data_path\": os.path.join(root_dir, \"test_datas_wordpiece.txt\"),\n",
        "            \"output_dir_path\":'/content/drive/MyDrive/롯데정보통신ai',\n",
        "            # \"cache_dir_path\": cache_dir,\n",
        "            # \"pretrained_model_name_or_path\": \"monologg/kobert\",\n",
        "            # \"label_vocab_data_path\": os.path.join(root_dir, \"label_vocab.txt\"),\n",
        "            \"num_labels\": 6,\n",
        "            \"max_length\": 64,\n",
        "            \"epoch\":1,\n",
        "            \"batch_size\":64,\n",
        "            }\n",
        "\n",
        "if (config[\"mode\"] == \"train\"):\n",
        "    train(config)\n",
        "else:\n",
        "    test(config)"
      ],
      "metadata": {
        "id": "PTr2KMC1qHyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OICQ-_55q2s2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}