{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JVsomEBP10zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> 강의 Review </h1>\n"
      ],
      "metadata": {
        "id": "QGZCP0atgAd5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSQrQ0O7omDo"
      },
      "outputs": [],
      "source": [
        "# 기존 distributed representation은 문맥에 따라 단어의 의미를 다르게 반영하지 못함\n",
        "\n",
        "# language model은 어떤 토큰 뒤에 다음 토큰이 나올 확률을 최대화하는 모델\n",
        "# 언어를 이해하는 모델을 만들자\n",
        "\n",
        "# 처음에 language model은 RNN으로 만들었음 (LSTM, GPU) -> ELMo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Anlalysis\n",
        "\n",
        "# 대용량 언어 모델인 BERT를 활용하여 입력된 문장의 긍부정을 분류하는 감성 분석 모델을 만드시오\n",
        "\n",
        "# 에제\n",
        "# 모델 입력 : 한글 문장\n",
        "# 모델 출력 : N or P\n",
        "# 데이터 형식 : 워드피스 토큰 열 \\t 레이블\n",
        "#            예제 : _아 _더 빙 . . _ 진짜 _짜 증 나 네요 _목소리    negative"
      ],
      "metadata": {
        "id": "k8HiXzONr6ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch 마다의 error를 평균내서 학습"
      ],
      "metadata": {
        "id": "_sQw06vAsvam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F24pbMKw8uT9"
      },
      "source": [
        "<h1>개인 구글 드라이브와 colab 연동</h1>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQXVAm_dgPoz",
        "outputId": "914763d6-2efe-4b45-bea9-a0ab320466a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4B-rbK8b3TW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3fe5a37-8ca9-48ac-cf41-acc77d3c37d0"
      },
      "source": [
        "# !pip install transformers\n",
        "# !pip install sentencepiece\n",
        "\n",
        "# root_dir = \"/content/drive/MyDrive/자연어처리/Sentiment_Analysis\"\n",
        "\n",
        "# import sys\n",
        "# sys.path.append(root_dir)\n",
        "\n",
        "\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "root_dir = '/content/drive/MyDrive/자연어처리수업조교/w3_semantic_analysis_movie_review'\n",
        "\n",
        "import sys\n",
        "sys.path.append(root_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall transformers==4.20.1\n",
        "# 출처: https://hyen4110.tistory.com/117 [Hyen4110:티스토리]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tEDB9IiGJ8QC",
        "outputId": "5bd7a37c-4453-4b39-ed4a-422903fe403e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.20.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from transformers==4.20.1)\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.20.1)\n",
            "  Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.17 (from transformers==4.20.1)\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0 (from transformers==4.20.1)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1 (from transformers==4.20.1)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17 (from transformers==4.20.1)\n",
            "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from transformers==4.20.1)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.20.1)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.27 (from transformers==4.20.1)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1)\n",
            "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers==4.20.1)\n",
            "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests->transformers==4.20.1)\n",
            "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->transformers==4.20.1)\n",
            "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->transformers==4.20.1)\n",
            "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, urllib3, typing-extensions, tqdm, regex, pyyaml, packaging, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2023.6.3\n",
            "    Uninstalling regex-2023.6.3:\n",
            "      Successfully uninstalled regex-2023.6.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.12.2 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.12.2 huggingface-hub-0.20.1 idna-3.6 numpy-1.26.2 packaging-23.2 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 tokenizers-0.12.1 tqdm-4.66.1 transformers-4.20.1 typing-extensions-4.9.0 urllib3-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "huggingface_hub",
                  "requests",
                  "tokenizers",
                  "tqdm",
                  "transformers",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdbZmdUt-d8I"
      },
      "source": [
        "# import os\n",
        "# from IPython.display import Image\n",
        "# Image(os.path.join(root_dir, \"BERT.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfWZLECjoJkn"
      },
      "source": [
        "<h1>BERT 모델을 이용한 감성분류</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5861idd25QB"
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "\n",
        "class SentimentClassifier(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(SentimentClassifier, self).__init__(config)\n",
        "\n",
        "        # BERT 모델\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        # 히든 사이즈\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        # 분류할 라벨의 개수\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "\n",
        "        # self.linear = nn.Linear(in_features=self.hidden_size, out_features=self.num_labels)\n",
        "        self.fc1 = nn.Linear(in_features=self.hidden_size, out_features = 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=self.num_labels)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # BERT 출력에서 CLS에 대응하는 벡터 표현 추출\n",
        "        # 선형 함수를 사용하여 예측 확률 분포로 변환\n",
        "\n",
        "        outputs = self.bert(input_ids=input_ids)\n",
        "\n",
        "        # (batch_size, max_length, hidden_size)\n",
        "        bert_output = outputs[0] # CLS의 output\n",
        "\n",
        "        # (batch_size, hidden_size)\n",
        "        cls_vector = bert_output[:,0,:]\n",
        "\n",
        "        # class_output : (batch_size, num_labels)\n",
        "        cls_output = self.fc1(cls_vector)\n",
        "        cls_output = self.relu(cls_output)\n",
        "\n",
        "        cls_output = self.fc2(cls_output)\n",
        "        # cls_output = self.softmax(cls_output)\n",
        "\n",
        "        return cls_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caSL4RyA1OOm"
      },
      "source": [
        "<h1>데이터 읽고 전처리 하기</h1>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_data(file_path)</b>\n",
        "  \"train_datas_wordpiece.txt\", \"test_datas_wordpiece.txt\" 파일을 읽기 위한 함수\n",
        "  \n",
        "  데이터 예시)\n",
        "    ▁아 ▁더 빙 . . ▁진짜 ▁짜 증 나 네요 ▁목소리 \\t negative\n",
        "  \n",
        "  read_file(file_path)\n",
        "  args\n",
        "    file_path : 읽고자 하는 데이터의 경로\n",
        "  return\n",
        "    datas : 영화 리뷰, 정답 라벨\n",
        "    \n",
        "    출력 예시)\n",
        "      datas = [\n",
        "        (['▁아', '▁더', '빙', '.', '.', '▁진짜', '▁짜', '증', '나', '네요', '▁목소리'], negative)\n",
        "\n",
        "        (...),\n",
        "        \n",
        "        ]\n",
        "      \n",
        "<b>2. read_vocab_data(vocab_data_path)</b>\n",
        "  \"label_vocab.txt\" 파일을 읽고 라벨을 indexing하기 위한 딕셔너리를 생성\n",
        "   \n",
        "  read_vocab_data(vocab_data_path)\n",
        "  args\n",
        "    vocab_data_path : 어휘 파일 경로\n",
        "  return  \n",
        "    term2idx : 라벨을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "    idx2term : index를 대응하는 라벨로 치환하기 위한 딕셔너리\n",
        "\n",
        "<b>3. convert_data2feature(datas, max_length, tokenizer, label2idx)</b>\n",
        "  입력 데이터를 고정된 길이로 변환 후 indexing\n",
        "  Tensor로 변환\n",
        "   \n",
        "  convert_data2feature(datas, max_length, tokenizer, label2idx)\n",
        "  args\n",
        "    datas : 영화 리뷰 데이터와 대응하는 정답 라벨을 갖고 있는 리스트\n",
        "    max_length : 입력의 최대 길이\n",
        "    tokenizer : electra tokenizer 객체\n",
        "    label2idx : 라벨을 대응하는 index로 치환하기 위한 딕셔너리\n",
        "  return\n",
        "    input_ids_features : 입력 문장에 대한 index sequence\n",
        "    label_id_features : 정답을 갖고 있는 리스트\n",
        "    \n",
        "  전처리 예시)\n",
        "    tokenizing된 리뷰 데이터['▁아', '▁더', '빙', '.', '.', '▁진짜', '▁짜', '증', '나', '네요', '▁목소리', ...]\n",
        "    input_ids : [2, 3360, 28709, 18, 18, 12704, 29334, ... ]\n",
        "    label_id : [1]\n",
        " </pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOGS8rse1ZZZ"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def read_data(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "        lines = inFile.readlines()\n",
        "\n",
        "    datas = []\n",
        "    for line in lines:\n",
        "        # 입력 데이터를 \\t을 기준으로 분리\n",
        "        pieces = line.strip().split(\"\\t\")\n",
        "\n",
        "        # 리뷰, 정답\n",
        "        input_sequence, label = pieces[0].split(\" \"), pieces[1]\n",
        "\n",
        "        datas.append((input_sequence, label))\n",
        "\n",
        "    return datas\n",
        "\n",
        "\n",
        "def read_vocab_data(vocab_data_path):\n",
        "    term2idx, idx2term = {},{}\n",
        "\n",
        "    with open(vocab_data_path, \"r\", encoding=\"utf8\") as inFile:\n",
        "        lines = inFile.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        term = line.strip()\n",
        "        term2idx[term] = len(term2idx)\n",
        "        idx2term[term2idx[term]] = term\n",
        "\n",
        "    return term2idx, idx2term\n",
        "\n",
        "\n",
        "def convert_data2feature(datas, max_length, tokenizer, label2idx):\n",
        "    input_ids_features, label_id_features = [], []\n",
        "\n",
        "    for input_sequence, label in datas:\n",
        "\n",
        "        # CLS, SEP 토큰 추가\n",
        "        tokens = [tokenizer.cls_token]\n",
        "        tokens += input_sequence\n",
        "        tokens = tokens[:max_length - 1]\n",
        "        tokens += [tokenizer.sep_token]\n",
        "\n",
        "        # word piece들을 대응하는 index로 치환\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # padding 생성\n",
        "        padding = [tokenizer._convert_token_to_id(tokenizer.pad_token)] * (max_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "\n",
        "        label_id = label2idx[label]\n",
        "\n",
        "        # 변환한 데이터를 각 리스트에 저장\n",
        "        input_ids_features.append(input_ids)\n",
        "        label_id_features.append(label_id)\n",
        "\n",
        "    # 변환한 데이터를 Tensor 객체에 담아 반환\n",
        "    input_ids_features = torch.tensor(input_ids_features, dtype=torch.long)\n",
        "    label_id_features = torch.tensor(label_id_features, dtype=torch.long)\n",
        "\n",
        "    return input_ids_features, label_id_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLIOIHT779c"
      },
      "source": [
        "<h1>BERT 모델 학습</h1>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_data(file_path) 함수를 사용하여 학습 데이터 읽기</b>\n",
        "\n",
        "<b>2. read_vocab_data(vocab_data_path) 함수를 사용하여 어휘 딕셔너리 생성</b>\n",
        "\n",
        "<b>3. convert_data2feature(datas, max_length, tokenizer, label2idx) 함수를 사용하여 데이터 전처리</b>\n",
        "\n",
        "<b>4. BERT 모델 객체 선언 후 사전 학습 파일 불러옴</b>\n",
        "\n",
        "<b>5. epoch 마다 학습한 모델 파일 저장</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsYLc2YK8eNc"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import (DataLoader, TensorDataset, RandomSampler)\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from transformers import BertConfig\n",
        "from tokenization_kobert import KoBertTokenizer\n",
        "# from transformers import KoBertTokenizer\n",
        "# from transformers import KoBertTokenizer\n",
        "\n",
        "def train(config):\n",
        "    # BERT config 객체 생성\n",
        "    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                             cache_dir=config[\"cache_dir_path\"])\n",
        "    setattr(bert_config, \"num_labels\", config[\"num_labels\"])\n",
        "\n",
        "    # BERT tokenizer 객체 생성\n",
        "    bert_tokenizer = KoBertTokenizer.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                                     cache_dir=config[\"cache_dir_path\"])\n",
        "\n",
        "    # 라벨 딕셔너리 생성\n",
        "    label2idx, idx2label = read_vocab_data(vocab_data_path=config[\"label_vocab_data_path\"])\n",
        "\n",
        "    # 학습 및 평가 데이터 읽기\n",
        "    train_datas = read_data(file_path=config[\"train_data_path\"])\n",
        "\n",
        "    # 입력 데이터 전처리\n",
        "    train_input_ids_features, train_label_id_features = convert_data2feature(datas=train_datas,\n",
        "                                                                             max_length=config[\"max_length\"],\n",
        "                                                                             tokenizer=bert_tokenizer,\n",
        "                                                                             label2idx=label2idx)\n",
        "\n",
        "    # 학습 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n",
        "    train_dataset = TensorDataset(train_input_ids_features, train_label_id_features)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config[\"batch_size\"],\n",
        "                                  sampler=RandomSampler(train_dataset))\n",
        "\n",
        "    # 사전 학습된 BERT 모델 파일로부터 가중치 불러옴\n",
        "    model = SentimentClassifier.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                                cache_dir=config[\"cache_dir_path\"], config=bert_config).cuda()\n",
        "\n",
        "    # loss를 계산하기 위한 함수\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 모델 학습을 위한 optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "    for epoch in range(config[\"epoch\"]):\n",
        "        model.train()\n",
        "\n",
        "        total_loss = []\n",
        "        for batch in train_dataloader:\n",
        "            batch = tuple(t.cuda() for t in batch)\n",
        "            input_ids, label_id = batch\n",
        "\n",
        "            # 역전파 단계를 실행하기 전에 변화도를 0으로 변경\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 모델 예측 결과\n",
        "            hypothesis = model(input_ids)\n",
        "\n",
        "            # loss 계산\n",
        "            loss = loss_func(hypothesis, label_id)\n",
        "\n",
        "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
        "            loss.backward()\n",
        "            # 모델 내부 각 매개변수 가중치 갱신\n",
        "            optimizer.step()\n",
        "\n",
        "            # batch 단위 loss 값 저장\n",
        "            total_loss.append(loss.data.item())\n",
        "\n",
        "        bert_config.save_pretrained(save_directory=config[\"output_dir_path\"])\n",
        "        model.save_pretrained(save_directory=config[\"output_dir_path\"])\n",
        "\n",
        "        print(\"Average loss : {}\".format(np.mean(total_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20OC-TY8FIFj"
      },
      "source": [
        "<h1>BERT 모델 평가</h1>\n",
        "\n",
        "<pre>\n",
        "<b>1. read_data(file_path) 함수를 사용하여 평가 데이터 읽기</b>\n",
        "\n",
        "<b>2. read_vocab_data(vocab_data_path) 함수를 사용하여 어휘 딕셔너리 생성</b>\n",
        "\n",
        "<b>3. convert_data2feature(datas, max_length, tokenizer, label2idx) 함수를 사용하여 데이터 전처리</b>\n",
        "\n",
        "<b>4. BERT 모델 객체 선언 후 기존에 학습한 모델 파일 불러옴</b>\n",
        "\n",
        "<b>5. 학습한 BERT 모델 평가</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORSC_y9Nto04"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import (DataLoader, TensorDataset, SequentialSampler)\n",
        "\n",
        "from transformers import BertConfig\n",
        "from tokenization_kobert import KoBertTokenizer\n",
        "#fix\n",
        "# from transformers import KoBertTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test(config):\n",
        "    # BERT config 객체 생성\n",
        "    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path=config[\"output_dir_path\"],\n",
        "                                             cache_dir=config[\"cache_dir_path\"])\n",
        "\n",
        "    # BERT tokenizer 객체 생성 (기존 BERT tokenizer 그대로 사용)\n",
        "    bert_tokenizer = KoBertTokenizer.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n",
        "                                                     cache_dir=config[\"cache_dir_path\"])\n",
        "\n",
        "    # 라벨 딕셔너리 생성\n",
        "    label2idx, idx2label = read_vocab_data(vocab_data_path=config[\"label_vocab_data_path\"])\n",
        "\n",
        "    # 평가 데이터 읽기\n",
        "    test_datas = read_data(file_path=config[\"test_data_path\"])\n",
        "    test_datas = test_datas[:100]\n",
        "\n",
        "    # 입력 데이터 전처리\n",
        "    test_input_ids_features, test_label_id_features = convert_data2feature(datas=test_datas,\n",
        "                                                                           max_length=config[\"max_length\"],\n",
        "                                                                           tokenizer=bert_tokenizer,\n",
        "                                                                           label2idx=label2idx)\n",
        "\n",
        "    # 평가 데이터를 batch 단위로 추출하기 위한 DataLoader 객체 생성\n",
        "    test_dataset = TensorDataset(test_input_ids_features, test_label_id_features)\n",
        "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config[\"batch_size\"],\n",
        "                                 sampler=SequentialSampler(test_dataset))\n",
        "\n",
        "    # 학습한 모델 파일로부터 가중치 불러옴\n",
        "    model = SentimentClassifier.from_pretrained(pretrained_model_name_or_path=config[\"output_dir_path\"],\n",
        "                                                cache_dir=config[\"cache_dir_path\"], config=bert_config).cuda()\n",
        "\n",
        "    model.eval()\n",
        "    total_hypothesis = []\n",
        "    total_label_id = []\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "        input_ids, label_id = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 모델 예측 결과\n",
        "            hypothesis = model(input_ids)\n",
        "            # 모델의 출력값에 softmax와 argmax 함수를 적용\n",
        "            hypothesis = torch.argmax(torch.softmax(hypothesis, dim=-1), dim=-1)\n",
        "\n",
        "        # Tensor를 리스트로 변경\n",
        "        hypothesis = hypothesis.cpu().detach().numpy().tolist()\n",
        "        label_id = label_id.cpu().detach().numpy().tolist()\n",
        "\n",
        "        total_hypothesis.extend(hypothesis)\n",
        "        total_label_id.extend(label_id)\n",
        "\n",
        "        for index in range(len(input_ids)):\n",
        "            input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids[index])\n",
        "            input_sequence = bert_tokenizer.convert_tokens_to_string(input_tokens[1:input_tokens.index(bert_tokenizer.sep_token)])\n",
        "            predict = idx2label[hypothesis[index]]\n",
        "            correct = idx2label[label_id[index]]\n",
        "\n",
        "            print(\"입력 : {}\".format(input_sequence))\n",
        "            print(\"출력 : {}, 정답 : {}\\n\".format(predict, correct))\n",
        "\n",
        "        print(\"Accuracy= {0:f}\\n\".format(accuracy_score(total_hypothesis, total_label_id)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    cache_dir = os.path.join(root_dir, \"cache\")\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "    config = {\"mode\": \"train\",\n",
        "              \"train_data_path\": os.path.join(root_dir, \"train_datas_wordpiece.txt\"),\n",
        "              \"test_data_path\": os.path.join(root_dir, \"test_datas_wordpiece.txt\"),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"cache_dir_path\": cache_dir,\n",
        "              \"pretrained_model_name_or_path\": \"monologg/kobert\",\n",
        "              \"label_vocab_data_path\": os.path.join(root_dir, \"label_vocab.txt\"),\n",
        "              \"num_labels\": 2,\n",
        "              \"max_length\": 142,\n",
        "              \"epoch\":10,\n",
        "              \"batch_size\":64,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "5HdhjnerIz80",
        "outputId": "b4af0ed9-6376-49da-f62e-f4e7d18e8270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-75118ccadd35>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKoBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'KoBertTokenizer' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbtyjwvtFxf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451c6c0c-f670-4771-f548-e890782a533b"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "if(__name__==\"__main__\"):\n",
        "    output_dir = os.path.join(root_dir, \"output\")\n",
        "    cache_dir = os.path.join(root_dir, \"cache\")\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "\n",
        "    config = {\"mode\": \"test\",\n",
        "              \"train_data_path\": os.path.join(root_dir, \"train_datas_wordpiece.txt\"),\n",
        "              \"test_data_path\": os.path.join(root_dir, \"test_datas_wordpiece.txt\"),\n",
        "              \"output_dir_path\":output_dir,\n",
        "              \"cache_dir_path\": cache_dir,\n",
        "              \"pretrained_model_name_or_path\": \"monologg/kobert\",\n",
        "              \"label_vocab_data_path\": os.path.join(root_dir, \"label_vocab.txt\"),\n",
        "              \"num_labels\": 2,\n",
        "              \"max_length\": 142,\n",
        "              \"epoch\":10,\n",
        "              \"batch_size\":64,\n",
        "              }\n",
        "\n",
        "    if(config[\"mode\"] == \"train\"):\n",
        "        train(config)\n",
        "    else:\n",
        "        test(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'KoBertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 굳 ᄏ\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : GDNTOPCLASSINTHECLUB\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 음악이 주가 된, 최고의 음악영화\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 진정한 쓰레기\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한cg남무 아 그립다 동사서독같은 영화가 이건 3류아류작이다\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 괜찮네요오랜만포켓몬스터[UNK]밌어요\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 한국독립영화의 한계 그렇게 아버지가 된다와 비교됨\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 청춘은 아름답다 그 아름다움은 이성을 흔들어 놓는다. 찰나의 아름다움을 잘 포착한 섬세하고 아름다운 수채화같은 퀴어영화이다.\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 눈에 보이는 반전이었지만 영화의 흡인력은 사라지지 않았다.\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : \"\"\"스토리, 연출, 연기, 비주얼 등 영화의 기본 조차 안된 영화에 무슨 평을 해. 이런 영화 찍고도 김문옥 감독은 \"\"\"\"내가 영화 경력이 몇OO인데 조무래기들이 내 영화를 평론해?\"\"\"\" 같은 마인드에 빠져있겠지?\"\"\"\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 소위 [UNK]문가라는 평점은 뭐냐?\n",
            "출력 : negative, 정답 : positive\n",
            "\n",
            "입력 : 최고!!!!!!!!!!!!!!!!\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 발연기 도저히 못보겠다 진짜 이렇게 연기를 못할거라곤 상상도 못했네\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 나이스\n",
            "출력 : negative, 정답 : positive\n",
            "\n",
            "입력 : 별 재미도없는거 우려먹어 .... 챔프에서 방송 몇번했더라 ? ᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏ\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : '13일의 금요일', '나이트메어'시리즈와 함께 가장 많은 시리즈를 양산해냈던 헬레이저 시리즈의 첫편. 작가의 상상력이 돋보이는 작품이며, 갈고리로 사지찢는 고어씬은 지금보더라도 상당히 잔인하고 충격적이다.\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 나름 교훈돋기는 하지만 어쩔수없이 저평점 받을수밖에 없는 저질섹스코미디\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 꽤 재밌게 본 영화였다!\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 민주화 시대의 억눌린 영혼의 관음적인 욕구 분출.인상적이다.\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 일본 천황이 미국으로부터 받은 면죄부의 긴박한 과정을 루즈하고 지저분하게 늘어놓았다.\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 괜히 나올 때 어미 알 건드려서 긁어 부스름 만들었다는 분이 저 아래 보이던데, 영화 제대로 안 봤네. 알이 딱 까지면서 새끼가 나오려 했음. 그냥 가면 그 놈 한테 당했을 것임. 한 놈, 두 놈 막 나올 게 뻔했으니 작살낼 수 밖에 없었다.\n",
            "출력 : negative, 정답 : positive\n",
            "\n",
            "입력 : 50번은 봤네요어[UNK] 이렇게 잘만들었을까\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 실화라서더욱아름답고[UNK]하네요...많이울었어요벌써4년이란시간이흘렀네요\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 이건뭐 노답이네 80~90년대 어린이 영화좀 보고 본받아라!\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 지금까지 본 영화중 마음이 가장 따뜻해지는 영화.\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 너무너무 재밌다\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 안보면 후회[UNK]...\n",
            "출력 : negative, 정답 : positive\n",
            "\n",
            "입력 : 평점1점도 주기싫어지는 영화 배우나 감독이라는 사람이나 영화판에서 안봐으면 한다 그리고 평점알바생들 너무 티난다\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 정말,진짜,표현할수 없는 영화...\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 엉성한 액션에, 시나리오는 왜그러지.. 막판에 대동단결??\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 101, 102.. 103은 못나올것 같다.\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : ....재미가없어요 시간이 아깝고\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 북괴는 우리의 주적일뿐이다.\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : ᄏᄏᄏ 난생처음 로그인하고 평점남기네요.. 개빡쳐서.. 알바들 속지마세요 이런 ᄀ [UNK]같은 시간낭비가. 아놔진짜평점 마이너스 별오만개\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 셰익스피어는 셰익스피어고 이영화는 이영화.\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : EBS 한국영화특선 해서 봤다.Biff 개막작 선정되서 [UNK]까 궁금 했었는데 봐도 이율 모르겠다...\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 뭐야??라는 말 밖에는...\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 이게 영화야?\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 애니는 일본이 갑인듯.\n",
            "출력 : negative, 정답 : positive\n",
            "\n",
            "입력 : 롭 코헨의 몰락의 OO점\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 감동적이다....\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 은빛날개의 조종사\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 마음이1을보신분들은 이해하시겟지만 2는망장입니다 졸작이죠 솔직히말해선;; 그이유는 예를들면 엽기적인그녀는 긴시간을두고 코메디 멜로를 그렷습니다 하지만 이영화는 단시간 10분안에 모든것을 소화하려는 욕심 코믹하다 3분채안돼갑자기 생뚱맞게 슬픈음악이깔리고\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 이런스타일 액션영화 굿굿!!\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 제임스 헷필드 50먹고 더 파워풀해졌어ᄏᄏᄏᄏ\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 돈만있으면 내가 이것보단 더잘만들겠따 ᄏ\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 레이토와 다미앙의 시원한 액션은 어디갔나 [UNK]\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 내 참, 이딴 걸 드라마라고.... 그냥 이건 세계보건기구에서 발암물질로 올려야 한다.\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 아 극장에서 아무 기대없이 들어갔다가 감탄하면서 보고 나왔는데. 2001년도 였구나 그 때가.\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 아이디어가 아주 좋다 재밌다\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 난 재밌던데 평점 왜케 낮지 \";\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 역사얘기보다 영화배우, 감독들들에게 더 관심이 간다\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "입력 : 몇년만에 아메리칸조크보고 웃어보는거지. 하아이맛이야바로\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 재밌다! 내용도 신선하고 의미도있으며 연기도 좋고 영상도좋다~~\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 여름이면 한결같이 생각나는 드라마\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 콩콩~~~~ᄏ[UNK]\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : ...으...뭐 의도는 좋아봤자 전달이 돼야지;\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 아빠가 낚여서 본적 있다나 슈래기 영화\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 엠비씨 무비채널에서 봤는데 또 봐도 엄청 [UNK]나네여 ..\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "Accuracy= 0.750000\n",
            "\n",
            "입력 : 어우.. 이게 진짜 무서웠어요?? 진짜로? 시간 아까웠습니다.\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 몇번을 봐도 볼때마다 재밌다\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 재미있게 잘봤습니다 후속편을 기다리게 만드네요\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 결말 뻔히 알면서 보는데도 너무 슬프고 많이 울었음.\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 한번본적은업지만재미있을것같다\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 성경에 보면 지나치게 의인도 되지 말고 지혜롭게 되지도 말라고 했다. 주인공의 지나친 착한남자 컴플렉스는 결국 모든걸 엉망으로 만들었다.직장에서 해고되고 부인에게 거부당하고,그 모든 터전을 잃을만큼 불륜으로 생긴 아이를 보러가는게 그리 중요했을까???\n",
            "출력 : negative, 정답 : positive\n",
            "\n",
            "입력 : 미친놈들 집합소네 연출력 빵점, 스토리 빵점. 구성빵점\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 하...작은 그림보고 기대했는데..수염..눈색깔도 뭐..이건 뭐 원작 비충실..[UNK]\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 볼 거 없을때 봐도 재미 없는 영화!\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 관객모독 관객모독 관객모독 관객모독\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 이런내용완전좋다[UNK]\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 쇼타군 넘 좋아 돈키호테 대박 재밌음\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 참 대단한것 같습니다천...재..?\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 볼만함\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 제시카 알바가 벗고 달려드는데 [UNK]까는게 말이 되냐?\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 솔직히 이건 C급 그 이하의 영화이긴 함 ᄒᄒ;\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 개성있는 몬스터들과 귀여운 여자아이가 만나 뿜어내는 매력에 퐁당 빠졌다\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 제목이 찰지구나ᄏᄏᄏ\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 1956년 작품이라는게 믿기지가 않을정도로 디테일하다! 안소니 퀸의 명연기 또한 물론~\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 아찔한 사랑 줄다리기???\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 재미있었다! 또봐야징ᄒ\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 중간에 화면이 좀 끊기는것 빼곤 넘 좋았어요~\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 이 영화를 말하는데 긴 단어는 필요없다. 재수없는 졸작 이거면 충분하다.\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 최고최고! 바다까지! 4차원 고양이 로봇 도라에몽과 함께하는 진구와 친구들의 미지 탐험 이야기!우주꺼지 갔음 좋겠당~\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 완전 [UNK]없음..\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 로코 굉장히 즐겨보는데, 이 영화는 좀 별로였다. 뭔가 사랑도 개그도 억지스런 느낌..\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 후속작 계획은 없나요..? [UNK]\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 엔딩때까지 음성 넣은 것은 오버의 극치다\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 단 두마디 '감동'\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 뭘 만든 건가?\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 전기톱은못들고다니는데 엔진톱이겠죠\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 완전 재밌엇는데 왜 평점이??\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 제임스 완이 내 목표임 [UNK]\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 1점고 아깝다. 개막장 영화의 원조라고나 할까.아내와 사별한 지 얼마나 지났다고 딴 여자들 만나고 다니다가 결국 맥 라이언한테 꽂힌 톰 행크스나약혼자 두고 톰 행크스랑 썸 타다가 결국엔 약혼자 버리고 톰행크스한테 쪼르르 달려가는 맥 라이언이나.\n",
            "출력 : negative, 정답 : negative\n",
            "\n",
            "입력 : 가끔 문득 생각나서 다시보는 영화..색감이 정말 예술이죠 강렬하고 화려하고..츠지야안나의 너무도 아름답던 리즈시절도 볼 수 있고..\n",
            "출력 : positive, 정답 : positive\n",
            "\n",
            "입력 : 걸작은 몇안되고 졸작들만 넘쳐난다.\n",
            "출력 : positive, 정답 : negative\n",
            "\n",
            "Accuracy= 0.820000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentimentClassifier Class의 forward 함수에서 fully connected layer를 한 층 더 쌓고 사이에 Relu함수를 사용함으로써 기존 성능인 80% 에서 82%로 향상시킬 수 있었다."
      ],
      "metadata": {
        "id": "tvh0u2l_8fZM"
      }
    }
  ]
}